# LostInSubSpace: The G Ã— S Decomposition of Transformer Robustness

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**A mechanistic interpretability study proving Transformer robustness splits into Geometry (G) and Slack (S).**

---

## Key Insight

Transformer robustness decomposes into two orthogonal factors:

- **Geometry (G)**: The attention routing (QK parameters) â€” determines *what* representations are possible
- **Slack (S)**: The activation magnitudes (V/MLP weights) â€” provides *margin* for noise tolerance

**The Proof**: Swapping QK parameters between two trained models causes catastrophic failure (99% â†’ 0%), even though both models solved the same task. This proves G *causally* constrains S.

---

## Experiments

| Phase | Question | Result |
|-------|----------|--------|
| **1** | Is G causal? | âœ… QK-swap causes 99.98% drop |
| **2** | Does G lock before S? | âœ… "Geometry Annealing" at step 4000 |
| **3** | Is S multi-dimensional? | âœ… Young G permits orthogonal S |
| **4** | Does it scale to 4L? | âš ï¸ Metastable dynamics emerge |
| **5** | Language modeling (8L)? | âœ… Works, but no grokking |
| **6** | Othello world models? | ğŸ”„ In progress (A100) |

---

## Quick Start

```bash
# Clone
git clone https://github.com/Mat-Tom-Son/LostInSubSpace.git
cd LostInSubSpace

# Install
pip install -r requirements.txt

# Run key experiment (G causality)
python clean_audit/experiments/exp_1_interleaved.py
```

---

## Repository Structure

```
LostInSubSpace/
â”œâ”€â”€ clean_audit/
â”‚   â”œâ”€â”€ lib/                    # Core utilities
â”‚   â”‚   â”œâ”€â”€ metrics.py          # G, S measurement
â”‚   â”‚   â”œâ”€â”€ clamps.py           # Sedation interventions
â”‚   â”‚   â”œâ”€â”€ othello_dataset.py  # Othello game simulator
â”‚   â”‚   â””â”€â”€ deep_transformer.py # Multi-layer model
â”‚   â”‚
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ phase_4/            # 4-Layer metastable dynamics
â”‚   â”‚   â”œâ”€â”€ phase_5/            # TinyStories (8L language)
â”‚   â”‚   â””â”€â”€ phase_6/            # Othello-GPT [NEW]
â”‚   â”‚
â”‚   â””â”€â”€ data/                   # Results (gitignored)
â”‚
â”œâ”€â”€ paper/
â”‚   â””â”€â”€ final_report.pdf        # Full paper
â”‚
â”œâ”€â”€ FINDINGS.md                 # Detailed research log
â””â”€â”€ README.md                   # You are here
```

---

## Phase 6: Othello-GPT (Current Focus)

Testing G Ã— S on a **world-model task** where the model must:
1. Track a hidden board state from move sequences
2. Predict legal moves

This bridges the gap between synthetic tasks (modular arithmetic) and messy real-world tasks (language).

**Run on A100:**
```bash
cd clean_audit
bash run_cloud.sh
```

---

## Citation

```bibtex
@article{lostinsubspace2026,
  title={LostInSubSpace: The G Ã— S Decomposition of Transformer Robustness},
  author={Thompson, Mat},
  year={2026},
  url={https://github.com/Mat-Tom-Son/LostInSubSpace}
}
```

---

## License

MIT
